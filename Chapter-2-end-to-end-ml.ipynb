{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e878d-b4b8-4d9b-8e03-56d727082b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ad2d6-6fcf-4aa0-85e2-56e0115f773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. understand the problem \n",
    "#2.decide which model to use \n",
    "#3.messurement matrix \n",
    "#4.extra about collabe\n",
    "#5.load data if you want to do it quit ofter just make a function for it i am not doing since it's already here. \n",
    "\n",
    "def load_housing_data():\n",
    "    tar_file=Path('datasets/housing.tgz')\n",
    "    return pd.read_csv(Path('datasets/housing/housing.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ca7f9-0bd9-4d24-be74-22b59b24f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345d2f3-042a-4274-845e-903539e7b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616b2fc-a6be-49c4-9001-e8e2865d361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a quick look at data how it organize and where to deal with it.\n",
    "                                \n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f7d0c-a8ef-4b83-89a1-f31f7e2d4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf280e-691a-459d-908e-9a8af2be4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33078e09-d898-4aa3-8d36-e320171e69f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing.hist(bins=50,figsize=(14,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac94bc-c62e-4a44-aafd-18243e6d7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 TRAIN TEST SPLIT\n",
    "# it's too early to use train test split but for the better understanding of data we have done this eralier ,\n",
    "# also we have done random split but in order to not introduct bias we have split strtigically\n",
    "#there are many ways to do this so we are exploring it in many cells.\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb137d-8c56-4433-848b-c57be840865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set = train_test_split(housing,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e2fee-fd0d-49e3-b862-1b02ad820ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d297a8-288e-4b02-92f2-884e657a4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zooming on median_income column with histogram.\n",
    "import numpy as np\n",
    "housing['median_income'].plot(kind='hist',bins=29,figsize=(12,6))\n",
    "labels=np.linspace(0,14,29).round(1)\n",
    "plt.xticks(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abfb04-099d-42dd-b54a-98c6d34254e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pd.cut() func to convert income column into categorical column.\n",
    "housing['income_cat']=pd.cut(housing['median_income'],bins=[0,1.5,3,4.5,6,np.infty],labels=['0-1.5','1.5-3','3-4.5','4.5-6','6-\\u221E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34713e61-ad78-480d-b1da-573b914ea664",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['income_cat'].value_counts().sort_index().plot(kind='bar',rot=0,grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459b36a-3a5b-4a20-8511-88863a8dd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's create more than two sets with same ratio. but random elements in each sets. also it devide same portion of the lable column or\n",
    "# column you want to. \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "spliter=StratifiedShuffleSplit(n_splits=10,test_size=0.2,random_state=42)\n",
    "list=[]\n",
    "for train_index,test_index in spliter.split(housing,housing['income_cat']):\n",
    "    # print('train index :',train_index)\n",
    "    # print('test index :',test_index)\n",
    "    train_set_n=housing.iloc[train_index]\n",
    "    test_set_n=housing.iloc[test_index]\n",
    "    # print('train set :',train_set)\n",
    "    # print('test set :',test_set)\n",
    "    list.append([train_set,test_set])\n",
    "\n",
    "train_set_1,test_set_1=list[0]\n",
    "# print(train_set_1)\n",
    "# print('test set' , test_set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ba36c-7e94-4ce6-b0d4-7a0ac99f9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you just want 1 set train test split will do the job for you \n",
    "train_set_n,test_set_n=train_test_split(housing,test_size=0.2,random_state=42,stratify=housing['income_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c153fb-8554-4aba-92d3-2aaa3fbc314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('overall proportion of income_cat column :\\n',(housing['income_cat'].value_counts().sort_index())/len(housing))\n",
    "print('\\ntrainset proportion of income_cat column :\\n',(train_set_n['income_cat'].value_counts().sort_index())/len(train_set_n))\n",
    "print('\\ntestset proportion of income_cat column :\\n',(test_set_n['income_cat'].value_counts().sort_index())/len(test_set_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d79ca3-5449-4d21-bb6a-d50b8c66aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we just created income_cat column to do this so let's drop it \n",
    "housing=housing.drop('income_cat',axis=1)\n",
    "train_set_n=train_set_n.drop('income_cat',axis=1)\n",
    "test_set_n=test_set_n.drop('income_cat',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f001f-6598-465a-8ad9-779c47953961",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing=train_set_n.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64519b-05b3-4a45-97b9-2671e54e208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['population'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6469ee5-1ad4-4966-936f-dcd910f96791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPLORATION OF DATA\n",
    "#now we really exploring the data and labels\n",
    "housing.plot(kind='scatter',x='longitude',y='latitude',alpha=0.5,s=housing['population']/10,label='population',colorbar=True,cmap='jet',\n",
    "            c=housing['median_house_value'],edgecolors='red',figsize=(20,10),legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35015a0-54f3-444c-ac10-e62e17982b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it's a nice thing to see the corr between labels and other features\n",
    "coormat=housing.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4dec0-90c6-4e32-b225-ee945f46a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "coormat['median_house_value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb9ddc-3fb6-46bf-b5f4-6f05910daa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's visulized the correlation using scatter_matrix func from pandas\n",
    "# housing.info()\n",
    "from pandas.plotting import scatter_matrix\n",
    "plot_col=['housing_median_age','total_rooms','median_income','median_house_value']\n",
    "scatter_matrix(housing[plot_col],figsize=(12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764f4ea-18f7-494c-8189-6b224b4572da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since there is only one feature meadian income seems to relete the house value let's plot a scatter plot for it \n",
    "housing.plot(kind='scatter',x='median_income',y='median_house_value',alpha=0.4)\n",
    "plt.yticks(np.linspace(0,500000,11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9674c-e895-437b-89a4-8b43f7728d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we can see in our df the rooms , bedrooms , poplution given based on district but if it's in per person it will make more sence.\n",
    "housing.head()\n",
    "housing['rooms_per_house']=housing['total_rooms']/housing['households']\n",
    "housing['bedroom_ratio']=housing['total_bedrooms']/housing['total_rooms']\n",
    "housing['people_per_house']=housing['population']/housing['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1befc760-5b77-4f4f-8abd-f746dc6d260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.corr(numeric_only=True)['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f0270-0608-4b82-aa04-bafd1409e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's create x and y for model\n",
    "housing_lable=train_set_n['median_house_value']\n",
    "housing=train_set_n.drop('median_house_value',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb3eb8-2568-4fe7-b1de-ebb1b5d91e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5504061-14ca-45e2-9482-0e73017884bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HANDLING WITH MISSING DATA\n",
    "#as you can see there are some missing values in total bedrooms column you can get rid of it using three methods of pandas \n",
    "# 1. drop the column containg missing values \n",
    "housing.drop('total_bedrooms',axis=1)\n",
    "#2. remove whole row which cointain missing values for total bedrooms\n",
    "housing.dropna(subset=['total_bedrooms'],inplace=True)\n",
    "housing.info()\n",
    "#3. this both methods at the end removing data from the df so we should be go with fill missing values with the median\n",
    "housing=train_set_n.drop('median_house_value',axis=1)\n",
    "median=housing['total_bedrooms'].median()\n",
    "housing['total_bedrooms'].fillna(median,inplace=True)\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e054c2a-b440-43e1-b3a2-28d2f547c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#but it is more mannual way and scarry too. so you can use the simpleimputer from sklearn to fill missing values in whole df by a method you want to \n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "#creating a instace of imputer\n",
    "imp = SimpleImputer(strategy='median')\n",
    "\n",
    "#imputer only work on numberical data so excluding the other datatype you can also use include argument with 'number'\n",
    "housing_num=housing.select_dtypes(exclude='object')\n",
    "housing_num.info()\n",
    "\n",
    "#then simply fit that df and it will generate the median for each column\n",
    "imp.fit(housing_num)\n",
    "print('median using imputer :\\n',imp.statistics_)\n",
    "print('median using numpy on df :\\n',housing_num.median().values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ab5df-6296-4204-b715-1795fbc24335",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=imp.transform(housing_num)\n",
    "housing_tr=pd.DataFrame(X,columns=housing_num.columns,index=housing_num.index)\n",
    "housing_tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1d83f-571d-4b04-9728-a2a5621ab3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HANDLING TEXT AND CATEGORICAL DATA\n",
    "#we are gone use ordinal encoder which convert this categories into values.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder=OrdinalEncoder()\n",
    "housing_cat=ordinal_encoder.fit_transform(train_set_n[['ocean_proximity']])\n",
    "housing_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ae957-f924-4e52-9b44-be241f8e925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see it's numbers are not releted here but model will recognized as releted so we use onehotencode to transform this into 0-1\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder=OneHotEncoder()\n",
    "housing_cat_1hot=one_hot_encoder.fit_transform(housing_cat)\n",
    "# print(housing_cat_1hot)\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a23ab5-7036-4e85-8f08-96636390d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#well it was a array which is not much meaning full so let's convert this array into dataframe.\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "housing_cat_arr=one_hot_encoder.fit_transform(train_set_n[['ocean_proximity']])\n",
    "housing_cat_df = pd.DataFrame(housing_cat_arr,columns=one_hot_encoder.get_feature_names_out(),index=train_set_n.index)\n",
    "housing_cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58af944-ec72-43db-8914-b4ba95893e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE SCALING AND TRASFORMATION \n",
    "# 1.using minmaxscaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmaxscaler = MinMaxScaler(feature_range=(-1,1))\n",
    "housing_num_minmax_scale=minmaxscaler.fit_transform(housing_num)\n",
    "\n",
    "# 2.using standrd scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardscaler = StandardScaler()\n",
    "housing_num_standard_scale=standardscaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb00483-b423-4a0a-af89-5328c59376ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tailed distribution - both method failed because both method will distroy the outlier or model predictions\n",
    "# Solution - try to make distribution symetric \n",
    "# Ways to do that - 1. positive features with tail to right -- replace feature with it's values of power between 0 to 1 ex (sqrt) \n",
    "# 2. heavy tail -- try to use log scale \n",
    "# 3. devide the values in bucket just like we did when create income category column \n",
    "# 4. multimodel distribution ⅰ.using bucket but must use OneHotIncoder \n",
    "# ⅱ. add a feature for each mode - achieving using radial basis function (RBF) \n",
    "# most comman is gaussian RBF : exp(-γ(x-35)²) -- as the values goes far from 35 it will decrease fast.\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "housing_age_rbf_35=rbf_kernel(train_set_n[['housing_median_age']],[[35]],gamma=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768abaa-144c-49a0-8aad-aaf742ffe43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are studying the transformation if we transform the label data how we understand the real values here inverse transform func come to play.\n",
    "scaler =StandardScaler()\n",
    "labels=scaler.fit_transform(housing_lable.to_frame())\n",
    "# print(labels)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(housing[['median_income']],labels)\n",
    "\n",
    "pred = model.predict(housing[['median_income']][:5])\n",
    "scaler.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8eb9cc-134c-4675-a7be-f843cb62e477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# well we can do all the things just by one function not need to go this long way \n",
    "\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "model =TransformedTargetRegressor(LinearRegression(),transformer=StandardScaler())\n",
    "model.fit(housing[['median_income']],housing_lable)\n",
    "\n",
    "model.predict(housing[['median_income']][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ad8a0-2104-4a2c-a3d2-8ac38782b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see how you can create you own tranfrom func. Many time you needs to create you own transformation func for various transformation\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "#example of use case of Function Trasformer\n",
    "log_transformer = FunctionTransformer(np.log,np.exp)\n",
    "log_values = log_transformer.transform(housing['housing_median_age'])\n",
    "# print(housing['housing_median_age'])\n",
    "log_values\n",
    "\n",
    "ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
    "ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))\n",
    "\n",
    "#you can also create new transfomer from a old transformer and give parameter of the old one in new as a fix values \n",
    "rbf_transformer = FunctionTransformer(rbf_kernel,\n",
    "kw_args=dict(Y=[[35.]], gamma=0.1))\n",
    "age_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab06ce2-6a70-486d-88b3-62e6af032e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just see how to use transform , but what if we also want to use fit and trasform both function here by inherting a class we can do that.\n",
    "\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.base import check_array,check_is_fitted\n",
    "\n",
    "class StandardScalerClone(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,with_mean=True):\n",
    "        self.with_mean=with_mean\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        X=check_array(X)\n",
    "        self.mean_=X.mean()\n",
    "        self.std_=X.std()\n",
    "        self.n_features_in_=X.shape[1]\n",
    "        return self\n",
    "        \n",
    "    def transform(self,X):\n",
    "        check_is_fitted(self)\n",
    "        X=check_array(X)\n",
    "        assert self.n_features_in_==X.shape[1]\n",
    "        if self.with_mean:\n",
    "            X=X-self.mean_\n",
    "        return X/self.std_\n",
    "\n",
    "#well the thing to note here is we don't use func like : inverse_transform(),get_feature_names_in(),get_features_name_out() but we have to \n",
    "# whenever we build own transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f1f46-52bf-47b3-ae87-30cc4e18b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler=StandardScalerClone()\n",
    "stdscaler.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eddc147-d4ed-4dd9-bd91-cbea773dee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e2596-8696-4ef4-b048-1bb133b422a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf26ed-ca87-4c2c-815e-815deb157709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also custom trasformer can use other estimator \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self # always return self!\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af441335-cbbb-4a00-be29-b52f28a41b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIPELINE\n",
    "# as you can see you have to do many transformation in right order here pipeline comes to play\n",
    "import sklearn as sklearn\n",
    "sklearn.set_config(display=\"diagram\")\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "imp_stdscaler_pip=Pipeline([('imputer',SimpleImputer(strategy='median')),('scaler',StandardScaler())])\n",
    "\n",
    "# things to note 1.name can be anything just two limits --unique names --not __ (double underscore)\n",
    "# 2.in pipeline you can pass as many as arguments you want but all must be trasfomer except last one it can be anything(predictor,estimator..)\n",
    "\n",
    "imp_stdscaler_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1450b-2959-4405-b225-812e917e76a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However if you don't want to use the names you can use make_pipeline() function \n",
    "# just using .fit() you can do fit and transform.\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "imp_stdscaler_pip2=make_pipeline(SimpleImputer(strategy='median'),StandardScaler())\n",
    "imp_stdscaler_pip2\n",
    "\n",
    "#if the last method is predictor you use .predict() - do all trasformation and predict.\n",
    "#if last method is trasfomer you use .transform() - do all transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ace892e-ce4e-4464-a1ec-1f038aa374e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipline also have get_feature_names_out() func which is usefull for creating df out of array.\n",
    "housing_tr_arr=imp_stdscaler_pip.fit_transform(housing_num)\n",
    "housing_tr_df=pd.DataFrame(housing_tr_arr,columns=imp_stdscaler_pip.get_feature_names_out(),index=housing_num.index)\n",
    "housing_tr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9965d-8822-48ee-b494-8b08a7993499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline also support indexing\n",
    "imp_stdscaler_pip[:-1]\n",
    "imp_stdscaler_pip.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bad932-6f02-48b6-9d13-d800938ebdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now as we have seen we have to handle cat and num columns seprate but what if some pipeline do that \n",
    "cat_pip=make_pipeline(SimpleImputer(strategy='most_frequent'),OneHotEncoder())\n",
    "\n",
    "housing=pd.read_csv('datasets/housing/housing.csv')\n",
    "num_col=housing.columns[:-1]\n",
    "cat_col=housing.columns[-1]\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "transformer=ColumnTransformer([('num_pip',imp_stdscaler_pip,num_col),('cat_pip',cat_pip,cat_col)])\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed2a79-6206-4932-82a1-3e04931f711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is still some function we can use instead of doing things manually\n",
    "from sklearn.compose import make_column_selector,make_column_transformer\n",
    "\n",
    "transformer1=make_column_transformer((imp_stdscaler_pip,make_column_selector(dtype_exclude='object')),\n",
    "                                    (cat_pip,make_column_selector(dtype_include='object')))\n",
    "\n",
    "housing_tr1=transformer1.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd688f-3050-40ba-9d6a-a7e8a41a24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we have seen how to use transformer and pipeline now let's make some functions to do this repeted things \n",
    "\n",
    "def column_ratio(X):\n",
    "    return X[:,[0]]/X[:,[1]]\n",
    "\n",
    "def ratio_name(function_transformer,feature_names_in):\n",
    "    return ['ratio']\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(SimpleImputer(strategy='median'),\n",
    "                         FunctionTransformer(column_ratio,feature_names_out=ratio_name),\n",
    "                         StandardScaler())\n",
    "\n",
    "log_pipeline=make_pipeline(SimpleImputer(strategy='median'),\n",
    "                           FunctionTransformer(np.log,feature_names_out='one-to-one'),\n",
    "                           StandardScaler())\n",
    "\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "StandardScaler())\n",
    "\n",
    "processing=ColumnTransformer([\n",
    "(\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "(\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "(\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "(\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n",
    "\"households\", \"median_income\"]),\n",
    "(\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "(\"cat\", cat_pip, make_column_selector(dtype_include=object)),\n",
    "],\n",
    "remainder=default_num_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71eee1a-2b7a-44a0-b811-4642c2b7bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_final=processing.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147e0c4-834a-440e-9a2b-c4dd77c73e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f266e8-3c3a-4d21-8aac-c3f0b4b17aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train_set_n['median_house_value']\n",
    "train_set_n.drop('median_house_value',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7a62f-e0f3-4659-b975-23fbeaa36119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT AND TRAIN A MODEL \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "housing_lable=housing['median_house_value']\n",
    "lin_pip = make_pipeline(processing,LinearRegression())\n",
    "lin_pip.fit(train_set_n,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1529dc-c2e3-465a-a55c-1584e5b2f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_origional=test_set_n['median_house_value']\n",
    "# test_set_n.drop('median_house_value',inplace=True,axis=1)\n",
    "pred = lin_pip.predict(train_set_n)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fc20e-0d2a-464d-b969-7c04ae95b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0116c82-10eb-474e-804e-1249dec0b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y,pred,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064801f2-a3cb-47af-90cb-292426d1b83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see the error is very high so you have to use other model to see which works better on this data\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dec_pip=make_pipeline(processing,DecisionTreeRegressor(random_state=42))\n",
    "\n",
    "dec_pip.fit(train_set_n,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b4c91-808e-4483-a65e-0b85c898a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=dec_pip.predict(train_set_n)\n",
    "mean_squared_error(y,pred,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd1c03-8281-498c-b59e-f5df1886ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have use whole data to predict the model but it is better to use train test split and even better cross validation \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_validation_dic=-cross_val_score(dec_pip,train_set_n,y,scoring=\"neg_root_mean_squared_error\",cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f27508d-3681-4085-8ed7-895a38c6dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cross_validation_dic).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed7b5e-cac4-472f-9620-168e3163d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see it perform same as linear regression model now let's try random forest model and see how it works on data \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "ran_pip=make_pipeline(processing,RandomForestRegressor(random_state=42))\n",
    "\n",
    "cross_validation_ran=-cross_val_score(ran_pip,train_set_n,y,scoring='neg_root_mean_squared_error',cv=10)\n",
    "cross_validation_ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29fb79-1741-4f5a-985e-11f554075a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cross_validation_ran).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6371f-7d2e-4eba-946d-a057a2256764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH CV - use for selecting best parameters for models AKA fine tuning \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ran_grid_pip=Pipeline([('preprocessing',processing),('random_forest',RandomForestRegressor(random_state=42))])\n",
    "\n",
    "param_grid = [\n",
    "{'preprocessing__geo__n_clusters': [5, 8, 10],\n",
    "'random_forest__max_features': [4, 6, 8]},\n",
    "{'preprocessing__geo__n_clusters': [10, 15],\n",
    "'random_forest__max_features': [6, 8, 10]},\n",
    "]\n",
    "\n",
    "grid_search=GridSearchCV(ran_grid_pip,param_grid=param_grid,scoring='neg_root_mean_squared_error',cv=3)\n",
    "grid_search.fit(train_set_n,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9a532-a699-408d-9d42-f7519f01bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35352f2-d835-43ec-872a-a4f34e348b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58908120-c4c6-496a-9f3c-4c552d1c9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result=pd.DataFrame(grid_search.cv_results_)\n",
    "grid_result\n",
    "\n",
    "grid_result.sort_values(by='mean_test_score',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503b574-c3ee-4345-9cae-7449dbd8c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search is usefull when you have many parameter to test it selects random combo . think when you have continues values it help there.\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\n",
    "'random_forest__max_features': randint(low=2, high=20)}\n",
    "\n",
    "random_search=RandomizedSearchCV(ran_grid_pip,param_distribs,scoring='neg_root_mean_squared_error',cv=3,random_state=42)\n",
    "random_search.fit(train_set_n,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e6148-15ca-4d79-bf5b-356aa5b55b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search.cv_results_).sort_values('mean_test_score',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98467470-e818-4ecc-8bda-60d911769898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV,HalvingRandomSearchCV\n",
    "#this both are use for more exploration of hyperparameter or for using better computation power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12641a-17f6-40d7-acf1-3ba76aa25618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chosing the best model exploring the model results \n",
    "final_model=random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008ba47-dd35-42dd-8ad4-46002b718c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see which feature is how much imporatant\n",
    "feature_imp=(final_model[\"random_forest\"].feature_importances_).round(2)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ef9b1-b890-431e-846b-079af2b42e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can get correspoding features name\n",
    "final_model_feature_names=final_model['preprocessing'].get_feature_names_out()\n",
    "pd.DataFrame(data=feature_imp,index=final_model_feature_names).sort_values(by=0,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef8b32c-8979-450e-b751-bff3b43e9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluting the model on test set \n",
    "\n",
    "X_test=test_set_n.drop('median_house_value',axis=1)\n",
    "y_test=test_set_n['median_house_value']\n",
    "\n",
    "y_pred=final_model.predict(X_test)\n",
    "mean_squared_error(y_test,y_pred,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25986c0f-4010-466a-a380-c3cde10a49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to messure error in more details \n",
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "squared_errors = (y_pred - y_test) ** 2\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n",
    "loc=squared_errors.mean(),\n",
    "scale=stats.sem(squared_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cdbe2-93b5-482d-b6d4-bbf4d1d68eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAUNCH MONITER AND MAINTAIN YOUR SYSTEM \n",
    "\n",
    "#saving the model \n",
    "import joblib\n",
    "\n",
    "joblib.dump(final_model,'Regresion_model_predict_house_price.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8bcc547-1fa0-4d51-bc4a-d43427128ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after this you can deploy model and make sure to maintain it.\n",
    "# chapter-2 done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e18a56-e7fd-40c6-9688-8a9dd6d2dbea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "homl3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
